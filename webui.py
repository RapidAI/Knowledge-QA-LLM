# -*- encoding: utf-8 -*-
# @Author: SWHL
# @Contact: liekkaskono@163.com
import importlib
import shutil
import time
import uuid
from pathlib import Path
from typing import Dict

import numpy as np
import streamlit as st

from knowledge_qa_llm.encoder import EncodeText, ErnieEncodeText
from knowledge_qa_llm.file_loader import FileLoader
from knowledge_qa_llm.utils import get_timestamp, logger, make_prompt, mkdir, read_yaml
from knowledge_qa_llm.vector_utils import DBUtils

config = read_yaml("knowledge_qa_llm/config.yaml")

st.set_page_config(
    page_title=config.get("title"),
    page_icon=":robot:",
)


def init_ui_parameters():
    st.session_state["params"] = {}
    param = config.get("Parameter")

    st.sidebar.markdown("### ğŸ›¶ å‚æ•°è®¾ç½®")

    param_max_length = param.get("max_length")
    max_length = st.sidebar.slider(
        "max_length",
        min_value=param_max_length.get("min_value"),
        max_value=param_max_length.get("max_value"),
        value=param_max_length.get("default"),
        step=param_max_length.get("step"),
        help=param_max_length.get("tip"),
    )
    st.session_state["params"]["max_length"] = max_length

    param_top = param.get("top_p")
    top_p = st.sidebar.slider(
        "top_p",
        min_value=param_top.get("min_value"),
        max_value=param_top.get("max_value"),
        value=param_top.get("default"),
        step=param_top.get("step"),
        help=param_top.get("tip"),
    )
    st.session_state["params"]["top_p"] = top_p

    param_temp = param.get("temperature")
    temperature = st.sidebar.slider(
        "temperature",
        min_value=param_temp.get("min_value"),
        max_value=param_temp.get("max_value"),
        value=param_temp.get("default"),
        step=param_temp.get("stemp"),
        help=param_temp.get("tip"),
    )
    st.session_state["params"]["temperature"] = temperature


def init_ui_db():
    st.sidebar.markdown("### ğŸ§» çŸ¥è¯†åº“")
    uploaded_files = st.sidebar.file_uploader(
        "default",
        accept_multiple_files=True,
        label_visibility="hidden",
        help="æ”¯æŒå¤šä¸ªæ–‡ä»¶çš„é€‰å–",
    )

    upload_dir = config.get("upload_dir")
    btn_upload = st.sidebar.button("ä¸Šä¼ æ–‡æ¡£å¹¶åŠ è½½")
    if btn_upload:
        time_stamp = get_timestamp()
        doc_dir = Path(upload_dir) / time_stamp

        tips("æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°å¹³å°ä¸­...", icon="â³")
        for file_data in uploaded_files:
            bytes_data = file_data.getvalue()

            mkdir(doc_dir)
            save_path = doc_dir / file_data.name
            with open(save_path, "wb") as f:
                f.write(bytes_data)
        tips("ä¸Šä¼ å®Œæ¯•ï¼")

        with st.spinner(f"æ­£åœ¨ä»{doc_dir}æå–å†…å®¹...."):
            all_doc_contents = file_loader(doc_dir)

        pro_text = "æå–è¯­ä¹‰å‘é‡..."
        batch_size = config.get("encoder_batch_size", 16)
        uid = str(uuid.uuid1())
        st.session_state["connect_id"] = uid
        for file_path, one_doc_contents in all_doc_contents.items():
            my_bar = st.sidebar.progress(0, text=pro_text)
            content_nums = len(one_doc_contents)
            all_embeddings = []
            for i in range(0, content_nums, batch_size):
                start_idx = i
                end_idx = start_idx + batch_size
                end_idx = content_nums if end_idx > content_nums else end_idx

                cur_contents = one_doc_contents[start_idx:end_idx]
                # è¶…è¿‡384ï¼Œå°±ä¼šæŠ¥é”™ï¼Œè¿™é‡ŒæŒ‰æ­¥é•¿ä¸º384ï¼Œåˆ†æ‰¹æ¬¡é€å…¥
                if not cur_contents:
                    continue

                print(cur_contents)
                print("-----")
                embeddings = embedding_extract(cur_contents)
                if embeddings is None or embeddings.size == 0:
                    continue
                all_embeddings.append(embeddings)
                # for one_content in cur_contents:
                #     len_content = len(one_content)
                #     import pdb

                #     pdb.set_trace()
                #     if len_content <= max_content_len:
                #         embeddings = embedding_extract(one_content)
                #         if embeddings is None or embeddings.size == 0:
                #             continue
                #         all_embeddings.append(embeddings)
                #     else:
                #         for j in range(0, len_content, max_content_len):
                #             s_content = j
                #             e_content = s_content + max_content_len
                #             e_content = (
                #                 len_content if e_content > len_content else e_content
                #             )

                #             part_content = one_content[s_content:e_content]
                #             embeddings = embedding_extract(part_content)
                #             if embeddings is None or embeddings.size == 0:
                #                 continue
                #             all_embeddings.append(embeddings)

                my_bar.progress(
                    end_idx / content_nums,
                    f"Extract {file_path} datas: [{end_idx}/{content_nums}]",
                )
            my_bar.empty()

            if all_embeddings:
                all_embeddings = np.vstack(all_embeddings)
                db_tools.insert(file_path, all_embeddings, one_doc_contents, uid)
            else:
                tips(f"ä»{file_path}æå–å‘é‡ä¸ºç©ºã€‚")

        shutil.rmtree(doc_dir.resolve())
        tips("ç°åœ¨å¯ä»¥æé—®é—®é¢˜äº†å“ˆï¼")

    clear_db_btn = st.sidebar.button("æ¸…ç©ºçŸ¥è¯†åº“")
    if clear_db_btn:
        db_tools.clear_db()
        tips("çŸ¥è¯†åº“å·²ç»è¢«æ¸…ç©ºï¼")

    if "connect_id" in st.session_state:
        had_files = db_tools.get_files(uid=st.session_state.connect_id)
    else:
        had_files = db_tools.get_files()

    st.session_state.had_file_nums = len(had_files) if had_files else 0
    if had_files:
        st.sidebar.markdown("å·²æœ‰æ–‡æ¡£:")
        st.sidebar.markdown("\n".join([f" - {v}" for v in had_files]))


@st.cache_resource
def init_encoder(encoder_name: str, **kwargs):
    if "ERNIEBot" in encoder_name:
        return ErnieEncodeText(**kwargs)
    return EncodeText(**kwargs)


def predict(
    text,
    search_res,
    model,
    custom_prompt=None,
):
    for file, content in search_res.items():
        content = "\n".join(content)
        one_context = f"**ä»ã€Š{file}ã€‹** æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼š \n{content}"
        bot_print(one_context, avatar="ğŸ“„")

        logger.info(f"Context:\n{one_context}\n")

    context = "\n".join(sum(search_res.values(), []))
    response, elapse = get_model_response(text, context, custom_prompt, model)

    print_res = f"**æ¨ç†è€—æ—¶:{elapse:.5f}s**"
    bot_print(print_res, avatar="ğŸ“„")
    bot_print(response)


def predict_only_model(text, model):
    params_dict = st.session_state["params"]
    response = model(text, history=None, **params_dict)
    bot_print(response)


def bot_print(content, avatar: str = "ğŸ¤–"):
    with st.chat_message("assistant", avatar=avatar):
        message_placeholder = st.empty()
        full_response = ""
        for chunk in content.split():
            full_response += chunk + " "
            time.sleep(0.05)
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)


def get_model_response(text, context, custom_prompt, model):
    params_dict = st.session_state["params"]

    s_model = time.perf_counter()
    prompt_msg = make_prompt(text, context, custom_prompt)
    logger.info(f"Final prompt: \n{prompt_msg}\n")

    response = model(prompt_msg, history=None, **params_dict)
    elapse = time.perf_counter() - s_model

    logger.info(f"Reponse of LLM: \n{response}\n")
    if not response:
        response = "æŠ±æ­‰ï¼Œæˆ‘å¹¶ä¸èƒ½æ­£ç¡®å›ç­”è¯¥é—®é¢˜ã€‚"
    return response, elapse


def tips(txt: str, wait_time: int = 2, icon: str = "ğŸ‰"):
    st.toast(txt, icon=icon)
    time.sleep(wait_time)


if __name__ == "__main__":
    title = config.get("title")
    version = config.get("version", "0.0.1")
    st.markdown(
        f"<h3 style='text-align: center;'>{title} v{version}</h3><br/>",
        unsafe_allow_html=True,
    )

    init_ui_parameters()

    file_loader = FileLoader()

    db_path = config.get("vector_db_path")
    db_tools = DBUtils(db_path)

    llm_module = importlib.import_module("knowledge_qa_llm.llm")
    llm_params: Dict[str, Dict] = config.get("LLM_API")

    menu_col1, menu_col2, menu_col3 = st.columns([1, 1, 1])
    select_model = menu_col1.selectbox("ğŸ¨LLM:", llm_params.keys())
    if "ERNIEBot" in select_model:
        with st.expander("LLM ErnieBot", expanded=True):
            opt_col1, opt_col2 = st.columns([1, 1])
            api_type = opt_col1.selectbox(
                "API Type(å¿…é€‰)",
                options=["aistudio", "qianfan", "yinian"],
                help="æä¾›å¯¹è¯èƒ½åŠ›çš„åç«¯å¹³å°",
            )
            access_token = opt_col2.text_input(
                "Access Token(å¿…å¡«) &nbsp;[å¦‚ä½•è·å¾—ï¼Ÿ](https://github.com/PaddlePaddle/ERNIE-Bot-SDK/blob/develop/docs/authentication.md)",
                "",
                help="ç”¨äºè®¿é—®åç«¯å¹³å°çš„access tokenï¼ˆå‚è€ƒä½¿ç”¨è¯´æ˜è·å–ï¼‰ï¼Œå¦‚æœè®¾ç½®äº†AKã€SKåˆ™æ— éœ€è®¾ç½®æ­¤å‚æ•°",
            )
            llm_params[select_model]["api_type"] = api_type

            if access_token:
                llm_params[select_model]["access_token"] = access_token

    MODEL_OPTIONS = {
        name: getattr(llm_module, name)(**params) for name, params in llm_params.items()
    }

    encoder_params = config.get("Encoder")
    select_encoder = menu_col2.selectbox("ğŸ§¬æå–å‘é‡æ¨¡å‹:", encoder_params.keys())
    if "ERNIEBot" in select_encoder:
        with st.expander("æå–è¯­ä¹‰å‘é‡ ErnieBot", expanded=True):
            opt_col1, opt_col2 = st.columns([1, 1])
            extract_api_type = opt_col1.selectbox(
                "API Type(å¿…é€‰)",
                options=["aistudio", "qianfan", "yinian"],
                help="æä¾›å¯¹è¯èƒ½åŠ›çš„åç«¯å¹³å°",
                key="Extract_type",
            )
            encoder_params[select_encoder]["api_type"] = extract_api_type

            extract_access_token = opt_col2.text_input(
                "Access Token(å¿…å¡«) &nbsp;[å¦‚ä½•è·å¾—ï¼Ÿ](https://github.com/PaddlePaddle/ERNIE-Bot-SDK/blob/develop/docs/authentication.md)",
                "",
                help="ç”¨äºè®¿é—®åç«¯å¹³å°çš„access tokenï¼ˆå‚è€ƒä½¿ç”¨è¯´æ˜è·å–ï¼‰ï¼Œå¦‚æœè®¾ç½®äº†AKã€SKåˆ™æ— éœ€è®¾ç½®æ­¤å‚æ•°",
                key="Extract_token",
            )
            if extract_access_token:
                encoder_params[select_encoder]["access_token"] = extract_access_token

    embedding_extract = init_encoder(select_encoder, **encoder_params[select_encoder])

    TOP_OPTIONS = [5, 10, 15]
    search_top = menu_col3.selectbox("ğŸ”æœç´¢ Top_K:", TOP_OPTIONS)

    init_ui_db()

    with st.expander("ğŸ’¡Prompt", expanded=False):
        text_area = st.empty()
        input_prompt = text_area.text_area(
            label="Input",
            max_chars=500,
            height=200,
            label_visibility="hidden",
            value=config.get("DEFAULT_PROMPT"),
            key="input_prompt",
        )

    input_txt = st.chat_input("é—®ç‚¹å•¥å§ï¼")
    if input_txt:
        with st.chat_message("user", avatar="ğŸ˜€"):
            st.markdown(input_txt)

        llm = MODEL_OPTIONS[select_model]

        if not input_prompt:
            input_prompt = config.get("DEFAULT_PROMPT")

        query_embedding = embedding_extract(input_txt)
        with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£..."):
            uid = st.session_state.get("connect_id", None)
            search_res, search_elapse = db_tools.search_local(
                query_embedding, top_k=search_top, uid=uid
            )

        if search_res is None:
            bot_print("ä»çŸ¥è¯†åº“ä¸­æŠ½å–ç»“æœä¸ºç©ºï¼Œç›´æ¥é‡‡ç”¨LLMçš„æœ¬èº«èƒ½åŠ›å›ç­”ã€‚", avatar="ğŸ“„")
            predict_only_model(input_txt, llm)
        else:
            logger.info(f"ä½¿ç”¨ {type(llm).__name__}")

            res_cxt = f"**Top{search_top}\n(å¾—åˆ†ä»é«˜åˆ°ä½ï¼Œè€—æ—¶:{search_elapse:.5f}s):** \n"
            bot_print(res_cxt, avatar="ğŸ“„")

            predict(
                input_txt,
                search_res,
                llm,
                input_prompt,
            )
