<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge-QA-LLM Documentation</title>
    <link>https://rapidai.github.io/Knowledge-QA-LLM/</link>
    <description>Recent content on Knowledge-QA-LLM Documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://rapidai.github.io/Knowledge-QA-LLM/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>概览</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</guid>
      <description>&amp;nbsp; 🧐 Knowledge QA LLM &amp;nbsp; Introduction link Questions &amp;amp; Answers based on local knowledge base + LLM. Reason: The idea of this project comes from Langchain-Chatchat I have used this project before, but it is not very flexible and deployment is not very friendly. Learn from the ideas in How to build a knowledge question answering system with a large language model, and try to use this as a practice.</description>
    </item>
    
    <item>
      <title>快速开始</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</guid>
      <description>1. 克隆整个项目到本地 link git clone https://github.com/RapidAI/Knowledge-QA-LLM.git 2. 安装运行环境 link cd Knowledge-QA-LLM pip install -r requirements.txt 3. 下载提取向量模型到本地 linkDownload the moka-ai/m3e-small model and put it in the assets/models/m3e-small directory. This model is used to vectorize text content.
4. 配置LLM API接口 linkSeparately configure the interface of chatglm2-6b, interface startup reference: ChatGLM2-6B API. The specific usage method Reference: knowledge_qa_llm/llm/chatglm2_6b.py
5. 更改config.yaml配置文件 linkWrite the deployed llm_api to the llm_api_url field in the configuration file knowledge_qa_llm/config.</description>
    </item>
    
    <item>
      <title>支持的LLM</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</guid>
      <description>已经支持的LLM API. link✔ ChatGLM2-6B
✔ BaiChuan-7B
✔ Qwen-7B
✔ llama2
✔ InternLM-7b</description>
    </item>
    
    <item>
      <title>自定义LLM API</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</guid>
      <description>info 该项目的LLM部分是独立的，用户可在**knowledge_qa_llm/llm**自定义配置所需的LLM接口。 下面以自定义支持InterLM-7b大模型为例，说明如何支持的。
1. 下载InterLM模型到本地，具体如何下载，参见internlm-7b。 link2. 编写模型的推理代码，这一点可以参考ChatGLM&amp;rsquo;s API的实现。只需要替换模型加载部分为InterLM的即可。具体如下： link from fastapi import FastAPI, Request from transformers import AutoTokenizer, AutoModel import uvicorn, json, datetime import torch DEVICE = &amp;#34;cuda&amp;#34; DEVICE_ID = &amp;#34;0&amp;#34; CUDA_DEVICE = f&amp;#34;{DEVICE}:{DEVICE_ID}&amp;#34; if DEVICE_ID else DEVICE def torch_gc(): if torch.cuda.is_available(): with torch.cuda.device(CUDA_DEVICE): torch.cuda.empty_cache() torch.cuda.ipc_collect() app = FastAPI() @app.post(&amp;#34;/&amp;#34;) async def create_item(request: Request): global model, tokenizer json_post_raw = await request.json() json_post = json.dumps(json_post_raw) json_post_list = json.loads(json_post) prompt = json_post_list.</description>
    </item>
    
    <item>
      <title>给作者加油</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</guid>
      <description>写在前面 linkI like open source and AI technology because I think open source and AI will bring convenience and help to people in need, and will also make the world a better place. By donating to these projects, you can join me in making AI bring warmth and beauty to more people.
我喜欢开源，喜欢AI技术，因为我认为开源和AI会为有需要的人带来方便和帮助，也会让这个世界变得更好。通过对这些项目的捐赠，您可以和我一道让AI为更多人带来温暖和美好。
知识星球RapidAI私享群 link这里的提问会优先得到回答和支持，也会享受到RapidAI组织后续持续优质的服务，欢迎大家的加入。
支付宝或微信打赏 (Alipay reward or WeChat reward) link通过支付宝或者微信给作者打赏，请写好备注。 Give the author a reward through Alipay or WeChat.</description>
    </item>
    
    <item>
      <title>更新日志</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</guid>
      <description>2023-09-07 v0.0.9 update: link Add tips when database is empty. 2023-08-29 v0.0.8 update: link Fixed missing embedding_extract Fixed default parameters of LLM 2023-08-11 v0.0.7 update: link Optimize layout, remove the plugin option, and put the extract vector model option on the home page. The tips are translated into English for easy communication. Add project logo:🧐 Update CLI module code. 2023-08-05 v0.0.6 update: link Adapt more llm_api, include online llm api, such ad ERNIE-Bot-Turbo.</description>
    </item>
    
  </channel>
</rss>
