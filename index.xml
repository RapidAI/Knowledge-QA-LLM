<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge-QA-LLM Documentation</title>
    <link>https://rapidai.github.io/Knowledge-QA-LLM/</link>
    <description>Recent content on Knowledge-QA-LLM Documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 14 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://rapidai.github.io/Knowledge-QA-LLM/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>æ¦‚è§ˆ</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</guid>
      <description>&amp;nbsp; ğŸ§ Knowledge QA LLM &amp;nbsp; Introduction link Questions &amp;amp; Answers based on local knowledge base + LLM. Reason: The idea of this project comes from Langchain-Chatchat I have used this project before, but it is not very flexible and deployment is not very friendly. Learn from the ideas in How to build a knowledge question answering system with a large language model, and try to use this as a practice.</description>
    </item>
    
    <item>
      <title>å¿«é€Ÿå¼€å§‹</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</guid>
      <description>1. å…‹éš†æ•´ä¸ªé¡¹ç›®åˆ°æœ¬åœ° link git clone https://github.com/RapidAI/Knowledge-QA-LLM.git 2. å®‰è£…è¿è¡Œç¯å¢ƒ link cd Knowledge-QA-LLM pip install -r requirements.txt 3. ä¸‹è½½æå–å‘é‡æ¨¡å‹åˆ°æœ¬åœ° linkæœ¬é¡¹ç›®ç›®å‰ä»¥Moka-AIçš„m3eæ¨¡å‹ä½œä¸ºæå–ç‰¹å¾å‘é‡çš„ä¸»è¦æ¨¡å‹ï¼Œå½“ç„¶å…¶ä»–æ¨¡å‹ï¼Œä¹Ÿå¯è‡ªè¡Œé…ç½®ã€‚
å°†moka-ai/m3e-smallä¸‹è½½ä¸‹æ¥æ”¾åˆ°assets/models/m3e-smallç›®å½•ä¸‹ï¼Œä¸‹è½½å‘½ä»¤å¦‚ä¸‹ï¼š
from sentence_transformers import SentenceTransformer # æŒ‡å®šcache_dirå³å¯ model = SentenceTransformer(&amp;#34;moka-ai/m3e-small&amp;#34;, cache_folder=&amp;#34;assets/models&amp;#34;) # éªŒè¯æ˜¯å¦å¯ç”¨ sentences = [&amp;#34;* Moka æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹ç”± MokaAI è®­ç»ƒå¹¶å¼€æºï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniem&amp;#34;,] embeddings = model.encode(sentences) for sentence, embedding in zip(sentences, embeddings): print(&amp;#34;Sentence:&amp;#34;, sentence) print(&amp;#34;Embedding:&amp;#34;, embedding) print(&amp;#34;&amp;#34;) 4. é…ç½®LLM APIæ¥å£ linké¦–å…ˆéœ€è¦å•ç‹¬åœ¨æœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹ï¼Œä»¥APIæ–¹å¼å¯åŠ¨ã€‚ä»¥ChatGLM-6Bä¸ºä¾‹ï¼Œå…·ä½“å¯å‚è€ƒChatGLM2-6B API
éšåï¼Œknowledge_qa_llm/llm/chatglm2_6b.pyæ˜¯è°ƒç”¨ä¸Šä¸€æ­¥LLMæ¥å£çš„ç±»ã€‚
å¦‚æœè‡ªå·±ä½¿ç”¨çš„LLMï¼Œæ²¡æœ‰è¯¥æ–‡ä»¶ï¼Œå¯è‡ªè¡Œå®ç°ï¼Œä¿è¯è¾“å…¥å’Œè¾“å‡ºä¸ç°æœ‰çš„ä¸€è‡´å³å¯ã€‚
5. æ›´æ”¹config.yamlé…ç½®æ–‡ä»¶ linkå°†è°ƒç”¨ChatGLM-6Bçš„llm_apiçš„urlå†™åˆ°knowledge_qa_llm/config.yamlé…ç½®æ–‡ä»¶ä¸­
LLM_API: ChatGLM2_6B: your_api 6. è¿è¡Œ link info streamlitæ¡†æ¶çš„å¯åŠ¨ï¼Œä¸å¯ä»¥ç”¨python webui.</description>
    </item>
    
    <item>
      <title>åœ¨çº¿demo</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/online_demo/</link>
      <pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/online_demo/</guid>
      <description>ç®€ä»‹ linkåœ¨çº¿demoæ˜¯åŸºäºç™¾åº¦çš„AI Studioå¹³å°æ­å»ºï¼ŒåŸºäºæ–‡å¿ƒä¸€è¨€å¤§æ¨¡å‹çš„æ¥å£æ­å»ºã€‚
å› ä¸ºè¯¥é¡¹ç›®æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¤§æ¨¡å‹çš„æ€»ç»“å’Œæå–èƒ½åŠ›ï¼Œä¸»æ‰“ç¦»çº¿ç§æœ‰éƒ¨ç½²ï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰ä¸€ä¸ªåœ¨çº¿demoä¾›å¤§å®¶æŸ¥çœ‹æ•ˆæœã€‚å› æ­¤æœ‰äº†åŸºäºæ–‡å¿ƒä¸€è¨€ç‰ˆçš„ ğŸ§ Knowledge QA LLMã€‚
Demoæºç  linkæ–‡å¿ƒä¸€è¨€ç‰ˆçš„æºç ä¸Githubä»“åº“ä»£ç ï¼Œæœ‰äº›è®¸åˆå…¥ï¼Œåç»­ä¼šåŒæ­¥ã€‚ åœ°å€ï¼š https://aistudio.baidu.com/projectdetail/6675380?contributionType=1
åœ¨çº¿Demo link notifications è¯¥Demoä¸»è¦ä¾§é‡æŸ¥çœ‹æ•ˆæœï¼Œè‡³äºå·¥ç¨‹åŒ–åˆ™å·®ä¸€äº›ã€‚ Knowledge-QA-LLM: https://aistudio.baidu.com/application/detail/7580</description>
    </item>
    
    <item>
      <title>æ”¯æŒçš„LLM</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</guid>
      <description>âœ” ChatGLM2-6B
âœ” BaiChuan-7B
âœ” Qwen-7B
âœ” llama2
âœ” InternLM-7b</description>
    </item>
    
    <item>
      <title>è‡ªå®šä¹‰LLM API</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</guid>
      <description>å¼•è¨€ link info è¯¥é¡¹ç›®çš„LLMéƒ¨åˆ†æ˜¯ç‹¬ç«‹çš„ï¼Œç”¨æˆ·å¯åœ¨ **knowledge_qa_llm/llm** è‡ªå®šä¹‰é…ç½®æ‰€éœ€çš„LLMæ¥å£ã€‚ ä¸‹é¢ä»¥è‡ªå®šä¹‰æ”¯æŒInterLM-7bå¤§æ¨¡å‹ä¸ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•æ”¯æŒçš„ã€‚å‰ææ˜¯æœ¬åœ°æ»¡è¶³éƒ¨ç½²LLMçš„æ¨ç†æ¡ä»¶ã€‚
æ­¥éª¤å¦‚ä¸‹ï¼š link1. éƒ¨ç½²LLMæ¨¡å‹åˆ°æœ¬åœ° linkå…·ä½“å¦‚ä½•ä¸‹è½½ï¼Œå‚è§Hugging Faceä¸­internlm-7bã€‚
2. ç¼–å†™æ¨¡å‹çš„éƒ¨ç½²æ¨ç†ä»£ç  linkè¿™ä¸€ç‚¹å¯ä»¥å‚è€ƒChatGLMAPIçš„å®ç°ã€‚åªéœ€è¦æ›¿æ¢æ¨¡å‹åŠ è½½éƒ¨åˆ†ä¸ºInternLMçš„å³å¯ã€‚å…·ä½“å¦‚ä¸‹ï¼š
from fastapi import FastAPI, Request from transformers import AutoTokenizer, AutoModel import uvicorn, json, datetime import torch DEVICE = &amp;#34;cuda&amp;#34; DEVICE_ID = &amp;#34;0&amp;#34; CUDA_DEVICE = f&amp;#34;{DEVICE}:{DEVICE_ID}&amp;#34; if DEVICE_ID else DEVICE def torch_gc(): if torch.cuda.is_available(): with torch.cuda.device(CUDA_DEVICE): torch.cuda.empty_cache() torch.cuda.ipc_collect() app = FastAPI() @app.post(&amp;#34;/&amp;#34;) async def create_item(request: Request): global model, tokenizer json_post_raw = await request.json() json_post = json.dumps(json_post_raw) json_post_list = json.</description>
    </item>
    
    <item>
      <title>ç»™ä½œè€…åŠ æ²¹</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</guid>
      <description>å†™åœ¨å‰é¢ linkI like open source and AI technology because I think open source and AI will bring convenience and help to people in need, and will also make the world a better place. By donating to these projects, you can join me in making AI bring warmth and beauty to more people.
æˆ‘å–œæ¬¢å¼€æºï¼Œå–œæ¬¢AIæŠ€æœ¯ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå¼€æºå’ŒAIä¼šä¸ºæœ‰éœ€è¦çš„äººå¸¦æ¥æ–¹ä¾¿å’Œå¸®åŠ©ï¼Œä¹Ÿä¼šè®©è¿™ä¸ªä¸–ç•Œå˜å¾—æ›´å¥½ã€‚é€šè¿‡å¯¹è¿™äº›é¡¹ç›®çš„æèµ ï¼Œæ‚¨å¯ä»¥å’Œæˆ‘ä¸€é“è®©AIä¸ºæ›´å¤šäººå¸¦æ¥æ¸©æš–å’Œç¾å¥½ã€‚
çŸ¥è¯†æ˜ŸçƒRapidAIç§äº«ç¾¤ linkè¿™é‡Œçš„æé—®ä¼šä¼˜å…ˆå¾—åˆ°å›ç­”å’Œæ”¯æŒï¼Œä¹Ÿä¼šäº«å—åˆ°RapidAIç»„ç»‡åç»­æŒç»­ä¼˜è´¨çš„æœåŠ¡ï¼Œæ¬¢è¿å¤§å®¶çš„åŠ å…¥ã€‚
æ”¯ä»˜å®æˆ–å¾®ä¿¡æ‰“èµ (Alipay reward or WeChat reward) linké€šè¿‡æ”¯ä»˜å®æˆ–è€…å¾®ä¿¡ç»™ä½œè€…æ‰“èµï¼Œè¯·å†™å¥½å¤‡æ³¨ã€‚ Give the author a reward through Alipay or WeChat.</description>
    </item>
    
    <item>
      <title>æ›´æ–°æ—¥å¿—</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</guid>
      <description>2023-09-07 v0.0.9 update: link Add tips when database is empty. 2023-08-29 v0.0.8 update: link Fixed missing embedding_extract Fixed default parameters of LLM 2023-08-11 v0.0.7 update: link Optimize layout, remove the plugin option, and put the extract vector model option on the home page. The tips are translated into English for easy communication. Add project logo:ğŸ§ Update CLI module code. 2023-08-05 v0.0.6 update: link Adapt more llm_api, include online llm api, such ad ERNIE-Bot-Turbo.</description>
    </item>
    
  </channel>
</rss>
