<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>博客 on Knowledge-QA-LLM Documentation</title>
    <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/</link>
    <description>Recent content in 博客 on Knowledge-QA-LLM Documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Nov 2022 12:36:15 +0000</lastBuildDate><atom:link href="https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>支持的LLM</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/supported_llm/</guid>
      <description>✔ ChatGLM2-6B
✔ BaiChuan-7B
✔ Qwen-7B
✔ llama2
✔ InternLM-7b</description>
    </item>
    
    <item>
      <title>自定义LLM API</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/blog/custom_llm_api/</guid>
      <description>引言 link info 该项目的LLM部分是独立的，用户可在 **knowledge_qa_llm/llm** 自定义配置所需的LLM接口。 下面以自定义支持InterLM-7b大模型为例，说明如何支持的。前提是本地满足部署LLM的推理条件。
步骤如下： link1. 部署LLM模型到本地 link具体如何下载，参见Hugging Face中internlm-7b。
2. 编写模型的部署推理代码 link这一点可以参考ChatGLMAPI的实现。只需要替换模型加载部分为InternLM的即可。具体如下：
from fastapi import FastAPI, Request from transformers import AutoTokenizer, AutoModel import uvicorn, json, datetime import torch DEVICE = &amp;#34;cuda&amp;#34; DEVICE_ID = &amp;#34;0&amp;#34; CUDA_DEVICE = f&amp;#34;{DEVICE}:{DEVICE_ID}&amp;#34; if DEVICE_ID else DEVICE def torch_gc(): if torch.cuda.is_available(): with torch.cuda.device(CUDA_DEVICE): torch.cuda.empty_cache() torch.cuda.ipc_collect() app = FastAPI() @app.post(&amp;#34;/&amp;#34;) async def create_item(request: Request): global model, tokenizer json_post_raw = await request.json() json_post = json.dumps(json_post_raw) json_post_list = json.</description>
    </item>
    
  </channel>
</rss>
