<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Documentation on Knowledge-QA-LLM Documentation</title>
    <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/</link>
    <description>Recent content in Documentation on Knowledge-QA-LLM Documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Oct 2022 02:21:15 +0000</lastBuildDate><atom:link href="https://rapidai.github.io/Knowledge-QA-LLM/docs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>概览</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/overview/</guid>
      <description> &amp;nbsp; 🧐 Knowledge QA LLM &amp;nbsp; 简介 link基于本地知识库+LLM的问答系统。该项目的思路是由langchain-ChatGLM启发而来。
缘由： 之前使用过这个项目，感觉不是太灵活，部署不太友好。 借鉴如何用大语言模型构建一个知识问答系统中思路，尝试以此作为实践。 优势： 整个项目为模块化配置，不依赖lanchain库，各部分可轻易替换，代码简单易懂。 除需要单独部署大模型接口外，其他部分用CPU即可。 支持常见格式文档，包括txt、md、pdf, docx, pptx, excel等等。当然，也可自定义支持其他类型文档。 整体流程 link解析文档并存储在数据库 linkflowchart LR A([Documents]) --ExtractText--&amp;gt; B([sentences]) B --Embeddings--&amp;gt; C([Embeddings]) C --Store--&amp;gt; D[(DataBase)] 检索并回答问题 linkflowchart LR E([Query]) --Embedding--&amp;gt; F([Embeddings]) --&amp;gt; H[(Database)] --Search--&amp;gt; G([Context]) E --&amp;gt; I([Prompt]) G --&amp;gt; I --&amp;gt; J([LLM]) --&amp;gt; K([Answer]) 使用的工具 link 文档分析: extract_office_content, rapidocr_pdf, rapidocr_onnxruntime 提取语义向量: moka-ai/m3e-small 向量存储: sqlite 向量检索: faiss UI搭建: streamlit&amp;gt;=1.25.0 </description>
    </item>
    
    <item>
      <title>快速开始</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/quickstart/</guid>
      <description>1. 克隆整个项目到本地 link git clone https://github.com/RapidAI/Knowledge-QA-LLM.git 2. 安装运行环境 link cd Knowledge-QA-LLM pip install -r requirements.txt 3. 下载提取向量模型到本地 link本项目目前以Moka-AI的m3e模型作为提取特征向量的主要模型，当然其他模型，也可自行配置。
将moka-ai/m3e-small下载下来放到assets/models/m3e-small目录下，下载命令如下：
from sentence_transformers import SentenceTransformer # 指定cache_dir即可 model = SentenceTransformer(&amp;#34;moka-ai/m3e-small&amp;#34;, cache_folder=&amp;#34;assets/models&amp;#34;) # 验证是否可用 sentences = [&amp;#34;* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem&amp;#34;,] embeddings = model.encode(sentences) for sentence, embedding in zip(sentences, embeddings): print(&amp;#34;Sentence:&amp;#34;, sentence) print(&amp;#34;Embedding:&amp;#34;, embedding) print(&amp;#34;&amp;#34;) 4. 配置LLM API接口 link首先需要单独在本地部署大模型，以API方式启动。以ChatGLM-6B为例，具体可参考ChatGLM2-6B API
随后，knowledge_qa_llm/llm/chatglm2_6b.py是调用上一步LLM接口的类。
如果自己使用的LLM，没有该文件，可自行实现，保证输入和输出与现有的一致即可。
5. 更改config.yaml配置文件 link将调用ChatGLM-6B的llm_api的url写到knowledge_qa_llm/config.yaml配置文件中
LLM_API: ChatGLM2_6B: your_api 6. 运行 link info streamlit框架的启动，不可以用python webui.</description>
    </item>
    
    <item>
      <title>在线demo</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/online_demo/</link>
      <pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/online_demo/</guid>
      <description>简介 link在线demo是基于百度的AI Studio平台搭建，基于文心一言大模型的接口搭建。
因为该项目核心在于利用大模型的总结和提取能力，主打离线私有部署，但是一直没有一个在线demo供大家查看效果。因此有了基于文心一言版的 🧐 Knowledge QA LLM。
Demo源码 link基于erniebot库来搭建的，如需使用，需要鉴权，提供Access Token，具体教程，参见：link
地址： https://aistudio.baidu.com/projectdetail/6675380?contributionType=1
在线Demo link notifications 该Demo主要侧重查看效果，至于工程化则差一些。 基于文心一言API的文档知识问答系统: https://aistudio.baidu.com/application/detail/8138</description>
    </item>
    
    <item>
      <title>给作者加油</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/sponsor/</guid>
      <description>写在前面 linkI like open source and AI technology because I think open source and AI will bring convenience and help to people in need, and will also make the world a better place. By donating to these projects, you can join me in making AI bring warmth and beauty to more people.
我喜欢开源，喜欢AI技术，因为我认为开源和AI会为有需要的人带来方便和帮助，也会让这个世界变得更好。通过对这些项目的捐赠，您可以和我一道让AI为更多人带来温暖和美好。
知识星球RapidAI私享群 link这里的提问会优先得到回答和支持，也会享受到RapidAI组织后续持续优质的服务，欢迎大家的加入。
支付宝或微信打赏 (Alipay reward or WeChat reward) link通过支付宝或者微信给作者打赏，请写好备注。 Give the author a reward through Alipay or WeChat.</description>
    </item>
    
    <item>
      <title>更新日志</title>
      <link>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rapidai.github.io/Knowledge-QA-LLM/docs/changelog/</guid>
      <description>2023-10-15 v0.0.10 update: link 当不能从文档中搜索到任何有效信息时，会直接调用模型本身的能力。 完善文档，添加超参数的解释 基于erniebot库，统一文心一言版本和仓库主分支版本 2023-09-07 v0.0.9 update: link 解决多人上传的文档，会被其他人搜到的问题 优化UI界面 2023-08-11 v0.0.7 update: link 优化布局，去掉插件选项，将提取向量模型选项放到主页部分 将提示语英语化，便于交流使用。 添加项目logo: 🧐 更新CLI使用代码 2023-08-05 v0.0.6 update: link 适配更多模型接口，包括在线大模型接口，例如文心一言 添加提取特征向量的状态提示 2023-08-04 v0.0.5 update: link 修复了插入数据库数据重复的问题。 2023-07-29 v0.0.4 update: link 基于streamlit==1.25.0优化UI 优化代码 录制UI GIF demo 2023-07-28 v0.0.3 update: link 完成文件解析部分 2023-07-25 v0.0.2 update: link 规范现有目录结构，更加紧凑，提取部分变量到config.yaml中 完善说明文档 </description>
    </item>
    
  </channel>
</rss>
